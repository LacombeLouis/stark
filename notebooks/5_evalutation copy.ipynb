{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_dict_from_file, all_int_in_set, flatten\n",
    "from utils_nlp import get_number_tokens\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dataset_name = os.getenv(\"dataset_name\")\n",
    "data_path = os.getenv(\"data_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "list_question_ids = [0, 2, 3, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 20, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 77, 78, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 107, 108]\n",
    "print(len(list_question_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_to_df(dict_):\n",
    "    df = pd.DataFrame(dict_)\n",
    "    return df\n",
    "\n",
    "def get_precision(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Get the precision of the model\n",
    "    \"\"\"\n",
    "    return len(y_pred.intersection(y_true)) / len(y_pred) if len(y_pred) > 0 else 0\n",
    "\n",
    "def get_recall(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Get the recall of the model\n",
    "    \"\"\"\n",
    "    return len(y_pred.intersection(y_true)) / len(y_true) if len(y_true) > 0 else 0\n",
    "\n",
    "def get_f1_score(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Get the F1 score of the model\n",
    "    \"\"\"\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "def get_response_in_context(response, context):\n",
    "    \"\"\"\n",
    "    Get the number of response in context\n",
    "    \"\"\"\n",
    "    context = context.lower()\n",
    "    count_in_context = 0\n",
    "    indx = 0\n",
    "    if len(response) == 0:\n",
    "        return 0, np.nan\n",
    "    for answer in response:\n",
    "        if answer.lower() in context:\n",
    "            count_in_context += 1\n",
    "            # find where answer in in context_\n",
    "            indx_ = context.index(answer.lower())\n",
    "            if indx_ > indx:\n",
    "                indx = indx_\n",
    "        else:\n",
    "            indx = 999999\n",
    "\n",
    "    if indx == 999999:\n",
    "        largest_indx = np.nan\n",
    "    else:\n",
    "        largest_indx = get_number_tokens(context[:indx])\n",
    "    return count_in_context, largest_indx\n",
    "\n",
    "def get_matching_reponse_id(response_id, response_name):\n",
    "    \"\"\"\n",
    "    Get the number of response in context\n",
    "    \"\"\"\n",
    "    return len(response_id)/len(response_name) if len(response_name) > 0 else np.nan\n",
    "\n",
    "def get_name_response(y_pred, y_true, response_name):\n",
    "    correct_responses = []\n",
    "    incorrect_responses = []\n",
    "    for i, item in enumerate(y_pred):\n",
    "        if item in y_true:\n",
    "            correct_responses.append(response_name[i])\n",
    "        else:\n",
    "            incorrect_responses.append(response_name[i])\n",
    "    return correct_responses, incorrect_responses\n",
    "\n",
    "\n",
    "def get_self_knowledge(y_pred, y_true, response_name, context):\n",
    "    cor_res, _ = get_name_response(y_pred, y_true, response_name)\n",
    "    response_in_context, _ = get_response_in_context(cor_res, context)\n",
    "    response_not_in_context = len(cor_res) - response_in_context\n",
    "\n",
    "    # add the responses from y_pred and y_true\n",
    "    number_responses = len(y_pred.union(y_true))\n",
    "    return response_not_in_context/number_responses if number_responses > 0 else np.nan\n",
    "\n",
    "\n",
    "def get_faithfulness(y_pred, y_true, response_name, context):    \n",
    "    response_in_context, _ = get_response_in_context(response_name, context)\n",
    "    number_responses = len(y_pred.union(y_true))\n",
    "    return response_in_context/number_responses if number_responses > 0 else np.nan\n",
    "\n",
    "\n",
    "def get_hallucination(y_pred, y_true, response_name, context):\n",
    "    _, incor_res = get_name_response(y_pred, y_true, response_name)\n",
    "    response_in_context, _ = get_response_in_context(incor_res, context)\n",
    "    response_not_in_context = len(incor_res) - response_in_context\n",
    "    number_responses = len(y_pred.union(y_true))\n",
    "    return response_not_in_context/number_responses if number_responses > 0 else np.nan\n",
    "\n",
    "\n",
    "def get_context_utilization(y_pred, y_true, response_name, context):\n",
    "    cor_res, incor_res = get_name_response(y_pred, y_true, response_name)\n",
    "    good_response_in_context, _ = get_response_in_context(cor_res, context)\n",
    "    bad_response_in_context, _ = get_response_in_context(incor_res, context)\n",
    "    total_ = good_response_in_context + bad_response_in_context\n",
    "    return good_response_in_context/total_ if total_ > 0 else np.nan\n",
    "\n",
    "\n",
    "def get_noise_sensitivity(y_pred, y_true, response_name, context):\n",
    "    _, incor_res = get_name_response(y_pred, y_true, response_name)\n",
    "    bad_response_in_context, _ = get_response_in_context(incor_res, context)\n",
    "    number_responses = len(y_pred.union(y_true))\n",
    "    return bad_response_in_context/number_responses if number_responses > 0 else np.nan\n",
    "\n",
    "\n",
    "def get_number_different_word_counter(context):\n",
    "    context = context.lower()\n",
    "    context = re.sub(r'[^\\w\\s]', '', context)\n",
    "    context = context.split()\n",
    "    return len(set(context))\n",
    "\n",
    "\n",
    "def get_count_text_in_context(text, context):\n",
    "    context = context.lower()\n",
    "    text = text.lower()\n",
    "    return context.count(text)\n",
    "\n",
    "\n",
    "def get_variability_context(response_name, context):\n",
    "    num_words = get_number_different_word_counter(context)\n",
    "    count_text = [get_count_text_in_context(text, context) for text in response_name]\n",
    "    return sum(count_text)/num_words if num_words > 0 else 0\n",
    "\n",
    "\n",
    "def get_rouge_score(correct_response, predicted_response):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    list_to_text1 = ' '.join(correct_response)\n",
    "    list_to_text2 = ' '.join(predicted_response)\n",
    "    scores = scorer.score(list_to_text1, list_to_text2)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def get_bleu_score(correct_response, predicted_response):\n",
    "    list_to_text1 = ' '.join(correct_response).split()\n",
    "    list_to_text2 = ' '.join(predicted_response).split()\n",
    "    scores = sentence_bleu([list_to_text1], list_to_text2, weights = [1])\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_meteor_score(correct_response, predicted_response):\n",
    "    list_to_text1 = ' '.join(correct_response).split()\n",
    "    list_to_text2 = ' '.join(predicted_response).split()\n",
    "    scores = meteor_score([list_to_text1], list_to_text2)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_dict_compiled_results(results_dict):\n",
    "    list_question_id = []\n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "    list_f1_score = []\n",
    "    list_answer_in_context = []\n",
    "    list_len_context = []\n",
    "    list_smallest_context_needed = []\n",
    "    list_matching_reponse_id = []\n",
    "    list_self_knowledge = []\n",
    "    list_faithfulness = []\n",
    "    list_hallucination = []\n",
    "    list_context_utilization = []\n",
    "    list_noise_sensitivity = []\n",
    "    list_variability_context = []\n",
    "\n",
    "    list_rouge_score = []\n",
    "    list_bleu_score = []\n",
    "    list_meteor_score = []\n",
    "\n",
    "\n",
    "    for item in results_dict.keys():\n",
    "        item_as_int = int(item)\n",
    "        if item_as_int in list_question_ids:\n",
    "            answer_id = set(results_dict[item][\"answer_id_\"])\n",
    "            answer_name = list(results_dict[item][\"answer_name\"])\n",
    "            response_id = set(flatten(results_dict[item]['answer_0'][\"matching_ids\"]))\n",
    "            # response_id = set(flatten(results_dict[item]['answer_0'][\"matching_ids_sim\"]))\n",
    "            response_name = list(results_dict[item]['answer_0'][\"response\"])\n",
    "            context_ = str(results_dict[item]['answer_0'][\"context\"])\n",
    "\n",
    "            if response_name == ['None']:\n",
    "                response_id = set([-1])\n",
    "                \n",
    "            list_question_id.append(item_as_int)\n",
    "            list_precision.append(get_precision(response_id, answer_id))\n",
    "            list_recall.append(get_recall(response_id, answer_id))\n",
    "            list_f1_score.append(get_f1_score(response_id, answer_id))\n",
    "            in_context, indx = get_response_in_context(answer_name, context_)\n",
    "            list_answer_in_context.append(in_context/len(answer_id))\n",
    "            list_smallest_context_needed.append(indx)\n",
    "            list_len_context.append(get_number_tokens(context_))\n",
    "            list_matching_reponse_id.append(get_matching_reponse_id(response_id, response_name))\n",
    "\n",
    "            list_self_knowledge.append(get_self_knowledge(response_id, answer_id, response_name, context_))\n",
    "            list_faithfulness.append(get_faithfulness(response_id, answer_id, response_name, context_))\n",
    "            list_hallucination.append(get_hallucination(response_id, answer_id, response_name, context_))\n",
    "            list_context_utilization.append(get_context_utilization(response_id, answer_id, response_name, context_))\n",
    "            list_noise_sensitivity.append(get_noise_sensitivity(response_id, answer_id, response_name, context_))\n",
    "            list_variability_context.append(get_variability_context(response_name, context_))\n",
    "            list_rouge_score.append(get_rouge_score(answer_name, response_name))\n",
    "            list_bleu_score.append(get_bleu_score(answer_name, response_name))\n",
    "            list_meteor_score.append(get_meteor_score(answer_name, response_name))\n",
    "\n",
    "    dict_ = {\n",
    "        \"question_id\": list_question_id,\n",
    "        \"precision\": list_precision,\n",
    "        \"recall\": list_recall,\n",
    "        \"f1_score\": list_f1_score,\n",
    "        \"answer_in_context\": list_answer_in_context,\n",
    "        \"len_context\": list_len_context,\n",
    "        \"smallest_context_needed\": list_smallest_context_needed,\n",
    "        \"matching_reponse_id\": list_matching_reponse_id,\n",
    "        \"self_knowledge\": list_self_knowledge,\n",
    "        \"faithfulness\": list_faithfulness,\n",
    "        \"hallucination\": list_hallucination,\n",
    "        \"context_utilization\": list_context_utilization,\n",
    "        \"noise_sensitivity\": list_noise_sensitivity,\n",
    "        \"variability_context\": list_variability_context,\n",
    "        \"rouge_score\": list_rouge_score,\n",
    "        \"bleu_score\": list_bleu_score,\n",
    "        \"meteor_score\": list_meteor_score\n",
    "    } \n",
    "    return dict_\n",
    "\n",
    "\n",
    "def get_metrics(dict_):\n",
    "    metrics_ = {}\n",
    "    answer_id = set(dict_[\"answer_id_\"])\n",
    "    answer_name = list(dict_[\"answer_name\"])\n",
    "    response_id = set(flatten(dict_['answer_0'][\"matching_ids\"]))\n",
    "    # response_id = set(flatten(dict_['answer_0'][\"matching_ids_sim\"]))\n",
    "    response_name = list(dict_['answer_0'][\"response\"])\n",
    "    context_ = str(dict_['answer_0'][\"context\"])\n",
    "\n",
    "    metrics_[\"precision\"] = get_precision(response_id, answer_id)\n",
    "    metrics_[\"recall\"] = get_recall(response_id, answer_id)\n",
    "    metrics_[\"f1_score\"] = get_f1_score(response_id, answer_id)\n",
    "    in_context, metrics_[\"smallest_context_needed\"] = get_response_in_context(answer_name, context_)\n",
    "    metrics_[\"answer_in_context\"] = in_context/len(answer_id)\n",
    "\n",
    "    metrics_[\"len_context\"] = get_number_tokens(context_)\n",
    "    metrics_[\"matching_reponse_id\"] = get_matching_reponse_id(response_id, response_name)\n",
    "\n",
    "    metrics_[\"self_knowledge\"] = get_self_knowledge(response_id, answer_id, response_name, context_)\n",
    "    metrics_[\"faithfulness\"] = get_faithfulness(response_id, answer_id, response_name, context_)\n",
    "    metrics_[\"hallucination\"] = get_hallucination(response_id, answer_id, response_name, context_)\n",
    "    metrics_[\"context_utilization\"] = get_context_utilization(response_id, answer_id, response_name, context_)\n",
    "    metrics_[\"noise_sensitivity\"] = get_noise_sensitivity(response_id, answer_id, response_name, context_)\n",
    "    metrics_[\"variability_context\"] = get_variability_context(response_name, context_)\n",
    "    metrics_[\"rouge_score\"] = get_rouge_score(answer_name, response_name)\n",
    "    metrics_[\"bleu_score\"] = get_bleu_score(answer_name, response_name)\n",
    "    metrics_[\"meteor_score\"] = get_meteor_score(answer_name, response_name)\n",
    "    return metrics_\n",
    "\n",
    "\n",
    "def compare_results_Q(Q_number, names):\n",
    "    results_dict = load_dict_from_file(data_path + names[0])\n",
    "    results_Q = results_dict[Q_number]\n",
    "\n",
    "    print(results_Q)\n",
    "\n",
    "    print(\"Questions: \", results_Q['question_'])\n",
    "    print(\"Answer ID: \", results_Q['answer_id_'])\n",
    "    print(\"True Answer: \", results_Q['answer_name'])\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    metrics_ = []\n",
    "\n",
    "    for item in names:\n",
    "        results_dict = load_dict_from_file(data_path + item)\n",
    "        results_Q = results_dict[Q_number]\n",
    "\n",
    "        print(\"Pred Answer : \", results_Q['answer_0']['response'])\n",
    "        print(\"Matching ID: \", results_Q['answer_0']['matching_ids'])\n",
    "\n",
    "        results = get_metrics(results_Q)\n",
    "        metrics_.append(results.values())\n",
    "        print(\"*\"*50)\n",
    "\n",
    "    columns = results.keys()\n",
    "    df = pd.DataFrame(metrics_, columns=columns).T\n",
    "    df.columns = names\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_files = ['04_09_RAG_v2.json', '16_10_both_low_top_k_no_text_tf_no_type_token7k.json', '16_10_both_low_top_k_no_text_tf_no_type_token7k_2.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from ../data/results/mini/04_09_RAG_v2.json\n",
      "Dictionary loaded from ../data/results/mini/16_10_both_low_top_k_no_text_tf_no_type_token7k.json\n",
      "Dictionary loaded from ../data/results/mini/16_10_both_low_top_k_no_text_tf_no_type_token7k_2.json\n"
     ]
    }
   ],
   "source": [
    "list_mean_values, list_std_values = [], []\n",
    "for name in compare_files:\n",
    "    results_dict = load_dict_from_file(data_path + name)\n",
    "    df = get_dict_to_df(get_dict_compiled_results(results_dict))\n",
    "    list_columns = df.columns\n",
    "    list_mean_values.append(np.mean(df, axis=0))\n",
    "    list_std_values.append(np.std(df, axis=0))\n",
    "\n",
    "df_mean = pd.DataFrame(list_mean_values, columns=list_columns).T\n",
    "df_std = pd.DataFrame(list_std_values, columns=list_columns).T\n",
    "\n",
    "df_mean.columns = compare_files\n",
    "df_std.columns = compare_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>04_09_RAG_v2.json</th>\n",
       "      <th>16_10_both_low_top_k_no_text_tf_no_type_token7k.json</th>\n",
       "      <th>16_10_both_low_top_k_no_text_tf_no_type_token7k_2.json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <td>54.717391</td>\n",
       "      <td>54.717391</td>\n",
       "      <td>54.717391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.359710</td>\n",
       "      <td>0.344328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.274819</td>\n",
       "      <td>0.377536</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.288509</td>\n",
       "      <td>0.318435</td>\n",
       "      <td>0.298962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_in_context</th>\n",
       "      <td>0.429891</td>\n",
       "      <td>0.713225</td>\n",
       "      <td>0.691486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_context</th>\n",
       "      <td>7176.782609</td>\n",
       "      <td>7218.565217</td>\n",
       "      <td>7219.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smallest_context_needed</th>\n",
       "      <td>1064.424242</td>\n",
       "      <td>1156.590164</td>\n",
       "      <td>1148.762712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matching_reponse_id</th>\n",
       "      <td>0.971571</td>\n",
       "      <td>0.974638</td>\n",
       "      <td>0.993577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_knowledge</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.598148</td>\n",
       "      <td>0.674540</td>\n",
       "      <td>0.665682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hallucination</th>\n",
       "      <td>0.030797</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_utilization</th>\n",
       "      <td>0.354651</td>\n",
       "      <td>0.371835</td>\n",
       "      <td>0.348112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noise_sensitivity</th>\n",
       "      <td>0.309094</td>\n",
       "      <td>0.381477</td>\n",
       "      <td>0.394046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variability_context</th>\n",
       "      <td>0.009557</td>\n",
       "      <td>0.150720</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_score</th>\n",
       "      <td>0.325128</td>\n",
       "      <td>0.344301</td>\n",
       "      <td>0.324761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_score</th>\n",
       "      <td>0.282491</td>\n",
       "      <td>0.298326</td>\n",
       "      <td>0.274389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meteor_score</th>\n",
       "      <td>0.250637</td>\n",
       "      <td>0.289667</td>\n",
       "      <td>0.262447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         04_09_RAG_v2.json  \\\n",
       "question_id                      54.717391   \n",
       "precision                         0.331522   \n",
       "recall                            0.274819   \n",
       "f1_score                          0.288509   \n",
       "answer_in_context                 0.429891   \n",
       "len_context                    7176.782609   \n",
       "smallest_context_needed        1064.424242   \n",
       "matching_reponse_id               0.971571   \n",
       "self_knowledge                    0.000000   \n",
       "faithfulness                      0.598148   \n",
       "hallucination                     0.030797   \n",
       "context_utilization               0.354651   \n",
       "noise_sensitivity                 0.309094   \n",
       "variability_context               0.009557   \n",
       "rouge_score                       0.325128   \n",
       "bleu_score                        0.282491   \n",
       "meteor_score                      0.250637   \n",
       "\n",
       "                         16_10_both_low_top_k_no_text_tf_no_type_token7k.json  \\\n",
       "question_id                                                      54.717391      \n",
       "precision                                                         0.359710      \n",
       "recall                                                            0.377536      \n",
       "f1_score                                                          0.318435      \n",
       "answer_in_context                                                 0.713225      \n",
       "len_context                                                    7218.565217      \n",
       "smallest_context_needed                                        1156.590164      \n",
       "matching_reponse_id                                               0.974638      \n",
       "self_knowledge                                                    0.000000      \n",
       "faithfulness                                                      0.674540      \n",
       "hallucination                                                     0.007246      \n",
       "context_utilization                                               0.371835      \n",
       "noise_sensitivity                                                 0.381477      \n",
       "variability_context                                               0.150720      \n",
       "rouge_score                                                       0.344301      \n",
       "bleu_score                                                        0.298326      \n",
       "meteor_score                                                      0.289667      \n",
       "\n",
       "                         16_10_both_low_top_k_no_text_tf_no_type_token7k_2.json  \n",
       "question_id                                                      54.717391       \n",
       "precision                                                         0.344328       \n",
       "recall                                                            0.325000       \n",
       "f1_score                                                          0.298962       \n",
       "answer_in_context                                                 0.691486       \n",
       "len_context                                                    7219.956522       \n",
       "smallest_context_needed                                        1148.762712       \n",
       "matching_reponse_id                                               0.993577       \n",
       "self_knowledge                                                    0.000000       \n",
       "faithfulness                                                      0.665682       \n",
       "hallucination                                                     0.005435       \n",
       "context_utilization                                               0.348112       \n",
       "noise_sensitivity                                                 0.394046       \n",
       "variability_context                                               0.126200       \n",
       "rouge_score                                                       0.324761       \n",
       "bleu_score                                                        0.274389       \n",
       "meteor_score                                                      0.262447       "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16_10_both_no_text_max_no_type: nlp_id=3, nlp_emb=5, q_emb=15, q_emb_top_5=3, check_ID>10\n",
    "\n",
    "16_10_both_low_top_k_no_text_max_no_type: nlp_id=2, nlp_emb=3, q_emb=15, q_emb_top_5=2, check_ID>10\n",
    "\n",
    "16_10_both_low_top_k_low_check_id_no_text_max_no_type: nlp_id=2, nlp_emb=3, q_emb=15, q_emb_top_5=2, check_ID >= 4\n",
    "\n",
    "16_10_both_low_top_k_no_q_emb_top_5_no_text_max_no_type: nlp_id=2, nlp_emb=3, q_emb=5, q_emb_top_5=NONE, check_ID>10\n",
    "\n",
    "16_10_sp_low_top_k_no_text_max_no_type: sp nlp_id=2, nlp_emb=3, q_emb=15, q_emb_top_5=2, check_ID>10\n",
    "\n",
    "16_10_both_low_top_k_no_text_max: nlp_id=2, nlp_emb=3, q_emb=15, q_emb_top_5=2, check_ID>10, \n",
    "\n",
    "16_10_both_low_top_k_no_text_strange_tdidf_no_type: using old tdidf nlp_id=2, nlp_emb=3, q_emb=15, q_emb_top_5=2, check_ID>10\n",
    "\n",
    "16_10_both_low_top_k_no_text_tf_no_type: tf nlp_id=2, nlp_emb=3, q_emb=15, q_emb_top_5=2, check_ID>10\n",
    "\n",
    "16_10_both_low_top_k_no_text_tf_no_type_token7k: tf nlp_id=2, nlp_emb=3, q_emb=15, q_emb_top_5=2, check_ID>10 token7k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from ../data/results/mini/16_10_both_low_top_k_no_text_tf_no_type_token7k.json\n",
      "54\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "results_dict = load_dict_from_file(data_path + \"16_10_both_low_top_k_no_text_tf_no_type_token7k.json\")\n",
    "\n",
    "dict_path = {}\n",
    "dict_link = {}\n",
    "for item in results_dict:    \n",
    "    context_ = str(results_dict[item]['answer_0'][\"context\"])\n",
    "    splits_ = context_.split(\"\\n\")\n",
    "    len_splits_ = [len(split.split(\"->\")) for split in splits_]\n",
    "    \n",
    "    # create a bool for have any split greater than 2\n",
    "    is_path = any([item > 3 for item in len_splits_])\n",
    "    if is_path:\n",
    "        dict_path[item] = results_dict[item]\n",
    "    else:\n",
    "        dict_link[item] = results_dict[item]\n",
    "\n",
    "print(len(dict_path))\n",
    "print(len(dict_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <td>60.489796</td>\n",
       "      <td>48.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.420918</td>\n",
       "      <td>0.289960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.412403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.355653</td>\n",
       "      <td>0.276024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_in_context</th>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.682171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_context</th>\n",
       "      <td>7175.448980</td>\n",
       "      <td>7267.697674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smallest_context_needed</th>\n",
       "      <td>818.529412</td>\n",
       "      <td>1582.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matching_reponse_id</th>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.968992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_knowledge</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.656755</td>\n",
       "      <td>0.694807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hallucination</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_utilization</th>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.304105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noise_sensitivity</th>\n",
       "      <td>0.312707</td>\n",
       "      <td>0.459843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variability_context</th>\n",
       "      <td>0.180173</td>\n",
       "      <td>0.117157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_score</th>\n",
       "      <td>0.403114</td>\n",
       "      <td>0.277281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_score</th>\n",
       "      <td>0.343002</td>\n",
       "      <td>0.247416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meteor_score</th>\n",
       "      <td>0.320102</td>\n",
       "      <td>0.254985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                path         link\n",
       "question_id                60.489796    48.139535\n",
       "precision                   0.420918     0.289960\n",
       "recall                      0.346939     0.412403\n",
       "f1_score                    0.355653     0.276024\n",
       "answer_in_context           0.740476     0.682171\n",
       "len_context              7175.448980  7267.697674\n",
       "smallest_context_needed   818.529412  1582.296296\n",
       "matching_reponse_id         0.979592     0.968992\n",
       "self_knowledge              0.000000     0.000000\n",
       "faithfulness                0.656755     0.694807\n",
       "hallucination               0.000000     0.015504\n",
       "context_utilization         0.429688     0.304105\n",
       "noise_sensitivity           0.312707     0.459843\n",
       "variability_context         0.180173     0.117157\n",
       "rouge_score                 0.403114     0.277281\n",
       "bleu_score                  0.343002     0.247416\n",
       "meteor_score                0.320102     0.254985"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_mean_values, list_std_values = [], []\n",
    "\n",
    "df = get_dict_to_df(get_dict_compiled_results(dict_path))\n",
    "list_columns = df.columns\n",
    "list_mean_values.append(np.mean(df, axis=0))\n",
    "list_std_values.append(np.std(df, axis=0))\n",
    "df = get_dict_to_df(get_dict_compiled_results(dict_link))\n",
    "list_columns = df.columns\n",
    "list_mean_values.append(np.mean(df, axis=0))\n",
    "list_std_values.append(np.std(df, axis=0))\n",
    "\n",
    "df_mean = pd.DataFrame(list_mean_values, columns=list_columns).T\n",
    "df_std = pd.DataFrame(list_std_values, columns=list_columns).T\n",
    "\n",
    "df_mean.columns = [\"path\", \"link\"]\n",
    "df_std.columns = [\"path\", \"link\"]\n",
    "\n",
    "df_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <td>60.489796</td>\n",
       "      <td>48.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.240578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.284496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.216383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_in_context</th>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.635659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_context</th>\n",
       "      <td>7175.408163</td>\n",
       "      <td>7270.720930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smallest_context_needed</th>\n",
       "      <td>801.617647</td>\n",
       "      <td>1620.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matching_reponse_id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_knowledge</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.644558</td>\n",
       "      <td>0.689754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hallucination</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_utilization</th>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.246306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noise_sensitivity</th>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.490363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variability_context</th>\n",
       "      <td>0.154750</td>\n",
       "      <td>0.093666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_score</th>\n",
       "      <td>0.412531</td>\n",
       "      <td>0.224744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_score</th>\n",
       "      <td>0.344239</td>\n",
       "      <td>0.194794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meteor_score</th>\n",
       "      <td>0.318584</td>\n",
       "      <td>0.198477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                path         link\n",
       "question_id                60.489796    48.139535\n",
       "precision                   0.435374     0.240578\n",
       "recall                      0.360544     0.284496\n",
       "f1_score                    0.371429     0.216383\n",
       "answer_in_context           0.740476     0.635659\n",
       "len_context              7175.408163  7270.720930\n",
       "smallest_context_needed   801.617647  1620.880000\n",
       "matching_reponse_id         1.000000     0.986258\n",
       "self_knowledge              0.000000     0.000000\n",
       "faithfulness                0.644558     0.689754\n",
       "hallucination               0.000000     0.011628\n",
       "context_utilization         0.435374     0.246306\n",
       "noise_sensitivity           0.309524     0.490363\n",
       "variability_context         0.154750     0.093666\n",
       "rouge_score                 0.412531     0.224744\n",
       "bleu_score                  0.344239     0.194794\n",
       "meteor_score                0.318584     0.198477"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_mean_values, list_std_values = [], []\n",
    "\n",
    "df = get_dict_to_df(get_dict_compiled_results(dict_path))\n",
    "list_columns = df.columns\n",
    "list_mean_values.append(np.mean(df, axis=0))\n",
    "list_std_values.append(np.std(df, axis=0))\n",
    "df = get_dict_to_df(get_dict_compiled_results(dict_link))\n",
    "list_columns = df.columns\n",
    "list_mean_values.append(np.mean(df, axis=0))\n",
    "list_std_values.append(np.std(df, axis=0))\n",
    "\n",
    "df_mean = pd.DataFrame(list_mean_values, columns=list_columns).T\n",
    "df_std = pd.DataFrame(list_std_values, columns=list_columns).T\n",
    "\n",
    "df_mean.columns = [\"path\", \"link\"]\n",
    "df_std.columns = [\"path\", \"link\"]\n",
    "\n",
    "df_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results_dict_rag:\n",
    "    response_name = list(results_dict_rag[item][\"answer_0\"][\"response\"])\n",
    "    response_id = list(results_dict_rag[item][\"answer_0\"][\"matching_ids\"])\n",
    "    if len(response_name) != len(response_id):\n",
    "        print(item)\n",
    "        print(results_dict_rag[item][\"answer_0\"][\"response\"])\n",
    "        print(results_dict_rag[item][\"answer_0\"][\"matching_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_rag = load_dict_from_file(data_path + \"04_09_RAG_v2.json\")\n",
    "results_rag = get_dict_compiled_results(results_dict_rag)\n",
    "\n",
    "results_dict_sp = load_dict_from_file(data_path + \"16_10_both_no_text_max_no_type.json\")\n",
    "results_sp_10k_hybrid = get_dict_compiled_results(results_dict_sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results_Q(\"8\", ['04_09_RAG_v2.json', '12_09_both_10k_hybrid099.json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_variable_name(variable):\n",
    "    names = [name for name in globals() if globals()[name] is variable]\n",
    "    if names:\n",
    "        return names[0]\n",
    "    return None\n",
    "\n",
    "def plot_by_question(data, name_of_metric=\"metric\", data2=None, where_=None):\n",
    "    if name_of_metric == \"metric\":\n",
    "        name_of_metric = get_variable_name(data)\n",
    "\n",
    "    if where_ is None:\n",
    "        where_ = np.ones_like(data, dtype=bool)\n",
    "\n",
    "    # Plotting\n",
    "    questions = np.array(list_question_ids)[where_]\n",
    "\n",
    "    plt.figure(figsize=(14, 3))\n",
    "\n",
    "    def plot_(data_, colors=[\"blue\", \"green\"], marker='o', name=None):\n",
    "        # Plot non-NaN points\n",
    "        is_nan = np.isnan(data_)\n",
    "        print(\"mean of data: \", np.nanmean(data_))\n",
    "        plt.scatter(questions[~is_nan], np.array(data_)[~is_nan], marker=marker, s=20, color=colors[0], label=f\"{name} Mean\")\n",
    "\n",
    "        print(f\"Number of NaN points: {np.sum(is_nan)}\")\n",
    "\n",
    "        if np.sum(is_nan) > 0:\n",
    "            # Explicitly plot NaN points as green dots\n",
    "            plt.scatter(questions[is_nan], np.zeros_like(questions[is_nan]), color=colors[1], marker=marker, s=20, label=\"NaN Points\")\n",
    "\n",
    "    plot_(np.array(data)[where_], colors=[\"blue\", \"green\"], marker='o', name='first')\n",
    "    if data2 is not None:\n",
    "        plot_(np.array(data2)[where_], colors=[\"purple\", \"darkgreen\"], marker='x', name='second')\n",
    "\n",
    "    plt.xlabel('Question', fontsize=14)\n",
    "    plt.ylabel(f\"{name_of_metric}\", fontsize=14)\n",
    "    plt.title(f\"Mean {name_of_metric} with Error Bars for Each Question\", fontsize=16)\n",
    "    plt.xticks(questions, rotation=90)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # put legend outside of the plot\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_ = np.where(np.array(results_rag[\"answer_in_context\"])<np.array(results_sp_10k_hybrid[\"answer_in_context\"]))[0]\n",
    "where_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_question(results_rag[\"precision\"], \"precision\", results_sp_10k_hybrid[\"precision\"], where_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_question(results_rag[\"recall\"], \"recall\", results_sp_10k_hybrid[\"recall\"], where_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_question(results_rag[\"rouge_score\"], \"rouge_score\", results_sp_10k_hybrid[\"rouge_score\"], where_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_question(results_rag[\"smallest_context_needed\"], \"smallest_context_needed\", results_sp_10k_hybrid[\"smallest_context_needed\"], where_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_question(results_rag[\"len_context\"], \"len_context\", results_sp_10k_hybrid[\"len_context\"], where_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
